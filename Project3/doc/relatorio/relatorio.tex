\documentclass[a4paper, 11pt, portuguese]{article}

% --- PACOTES ESSENCIAIS ---
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{babel}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[
    colorlinks=true,
    linkcolor=blue,
    citecolor=blue,
    urlcolor=blue
]{hyperref}
\usepackage{geometry}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{float}
\usepackage{tcolorbox}
\usepackage{multirow}
\usepackage{siunitx}

% --- CONFIGURAÇÕES ---
\geometry{
 a4paper,
 margin=2.5cm,
}

% Estilo para blocos de código C++
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codeblue}{rgb}{0,0,0.6}
\definecolor{codegreen}{rgb}{0,0.6,0}
\lstdefinestyle{cppstyle}{
    language=C++,
    basicstyle=\ttfamily\footnotesize,
    commentstyle=\color{codegray}\itshape,
    keywordstyle=\color{codeblue}\bfseries,
    stringstyle=\color{codegreen},
    numberstyle=\tiny\color{codegray},
    breaklines=true,
    frame=single,
    captionpos=b,
    showstringspaces=false,
    tabsize=4,
    numbers=left,
    stepnumber=1,
    numbersep=5pt,
    backgroundcolor=\color{white},
    literate={á}{{\'a}}1 {é}{{\'e}}1 {í}{{\'i}}1 {ó}{{\'o}}1 {ú}{{\'u}}1
             {Á}{{\'A}}1 {É}{{\'E}}1 {Í}{{\'I}}1 {Ó}{{\'O}}1 {Ú}{{\'U}}1
             {â}{{\^a}}1 {ê}{{\^e}}1 {ô}{{\^o}}1
             {Â}{{\^A}}1 {Ê}{{\^E}}1 {Ô}{{\^O}}1
             {ã}{{\~a}}1 {õ}{{\~o}}1
             {Ã}{{\~A}}1 {Õ}{{\~O}}1
             {ç}{{\c{c}}}1 {Ç}{{\c{C}}}1,
}
\lstset{style=cppstyle}

\selectlanguage{portuguese}

% --- INFORMAÇÕES DO DOCUMENTO ---
\title{
    \includegraphics[width=0.4\textwidth]{imagens/ua.pdf} \\ \vspace{1.5cm}
    \textbf{Relatório do Trabalho Laboratorial nº 3} \\
    \large Compressão Sem Perdas de Pesos de Modelos de Linguagem \\
    \vspace{0.5cm}
    \normalsize Informação e Codificação (2025/26)
}
\author{
    \textbf{Pedro Miguel Miranda de Melo} (114208) \\
    \textbf{Rúben Cardeal Costa} (114190) \\
    \textbf{Hugo Marques Dias} (114142) \\
    \vspace{0.5cm}
    \textit{Departamento de Eletrónica, Telecomunicações e Informática (DETI)} \\
    \textit{Universidade de Aveiro}
}
\date{Janeiro de 2025}

% --- INÍCIO DO DOCUMENTO ---
\begin{document}

\maketitle
\thispagestyle{empty}

\newpage
\tableofcontents
\newpage

% ----------------------------------------------------------------------------------
% SECÇÃO 1: INTRODUÇÃO
% ----------------------------------------------------------------------------------
\section{Introdução}

\subsection{Contexto e Motivação}
Os Modelos de Linguagem de Grande Escala (\textit{Large Language Models} -- LLMs) representam um dos avanços mais significativos na área da inteligência artificial nos últimos anos. Contudo, a sua utilização prática enfrenta desafios consideráveis relacionados com o armazenamento e distribuição dos ficheiros de pesos, que frequentemente atingem dimensões na ordem dos gigabytes. A compressão eficiente destes ficheiros é, portanto, uma área de investigação com relevância prática imediata.

\subsection{Objetivos do Trabalho}
O presente relatório descreve o desenvolvimento de um codec especializado para a compressão sem perdas (\textit{lossless}) do ficheiro \texttt{model.safetensors}, que contém os parâmetros do modelo Qwen2-0.5B disponibilizado pela Alibaba Cloud. Com aproximadamente 942 MB, este ficheiro constitui um caso de estudo representativo dos desafios de compressão de pesos de LLMs.

Os objetivos específicos deste trabalho são:
\begin{enumerate}
    \item \textbf{Maximizar a taxa de compressão} através de uma análise profunda da estrutura e estatística dos dados;
    \item \textbf{Manter tempos de processamento competitivos} face aos compressores de uso geral;
    \item \textbf{Controlar o consumo de memória} para permitir a execução em sistemas com recursos limitados;
    \item \textbf{Oferecer múltiplos pontos de operação} que permitam ao utilizador escolher o compromisso ideal entre compressão e velocidade.
\end{enumerate}

\subsection{Abordagem Metodológica}
A estratégia adotada baseia-se numa análise aprofundada da estrutura do formato BF16 (\textit{Brain Floating Point 16}), que revelou características estatísticas marcadamente distintas entre os bytes mais significativos (MSB) e menos significativos (LSB) de cada valor. Esta descoberta fundamental conduziu ao desenvolvimento de uma arquitetura \textit{split-stream} que processa cada canal de forma independente e otimizada para as suas características específicas.

\subsection{Estrutura do Relatório}
O relatório está organizado da seguinte forma: a Secção~\ref{sec:analise} apresenta a análise e caracterização da fonte de dados; a Secção~\ref{sec:benchmark} documenta o \textit{benchmarking} de compressores existentes; a Secção~\ref{sec:implementacao} detalha a implementação do codec; a Secção~\ref{sec:resultados} apresenta e discute os resultados experimentais; e a Secção~\ref{sec:conclusoes} sintetiza as principais conclusões e contribuições.

O código-fonte completo está disponível em: \url{https://github.com/Rubenc1234/IC_miniP1/tree/main/Project3}.

% ----------------------------------------------------------------------------------
% SECÇÃO 2: ANÁLISE E CARACTERIZAÇÃO DA FONTE
% ----------------------------------------------------------------------------------
\section{Análise e Caracterização da Fonte}
\label{sec:analise}

O desenho de um codec eficiente exige uma compreensão profunda da natureza estatística da fonte de informação. Esta secção detalha a análise teórica e experimental realizada sobre o ficheiro \texttt{model.safetensors}, desde a sua estrutura de alto nível até às propriedades estatísticas dos seus componentes individuais.

\subsection{Estrutura do Ficheiro SafeTensors}

O formato SafeTensors, desenvolvido pela Hugging Face, é um formato binário otimizado para o armazenamento seguro de tensores. A estrutura do ficheiro é composta por:

\begin{enumerate}
    \item \textbf{Cabeçalho de Tamanho} (8 bytes): Um inteiro de 64 bits em formato \textit{little-endian} que indica o tamanho do cabeçalho JSON;
    \item \textbf{Cabeçalho JSON} (variável): Metadados que descrevem cada tensor, incluindo nome, tipo de dados (\textit{dtype}), dimensões e \textit{offsets} no \textit{payload};
    \item \textbf{Payload Binário} (restante): Dados dos tensores armazenados de forma contígua.
\end{enumerate}

A extração e análise do cabeçalho JSON revelou que os pesos estão armazenados no formato \textbf{BF16} (\textit{Brain Floating Point 16}), uma representação numérica de 16 bits desenvolvida pelo Google para aplicações de \textit{machine learning}.

\subsection{Análise do Formato BF16}

Ao contrário de inteiros de 16 bits, onde a distribuição de bits pode ser relativamente uniforme, o formato BF16 possui uma semântica específica que influencia diretamente as suas propriedades estatísticas:

\begin{itemize}
    \item \textbf{1 bit de Sinal ($S$)}: Indica se o valor é positivo ou negativo;
    \item \textbf{8 bits de Expoente ($E$)}: Representam a magnitude do valor numa escala logarítmica;
    \item \textbf{7 bits de Mantissa ($M$)}: Representam a precisão fracionária do valor.
\end{itemize}

Numa organização \textit{little-endian}, o byte menos significativo (LSB) contém os 7 bits da mantissa mais o bit menos significativo do expoente, enquanto o byte mais significativo (MSB) contém o bit de sinal e os 7 bits mais significativos do expoente.

Esta estrutura sugere a existência de correlações não-lineares e localizadas que uma análise puramente sequencial (byte-a-byte) poderá não capturar eficazmente. A hipótese de trabalho formulada nesta fase foi que os dois bytes de cada valor BF16 apresentariam características estatísticas distintas, justificando um tratamento diferenciado.

\subsection{Limites Teóricos: Entropia de Shannon}

O limite teórico fundamental para a compressão sem perdas é dado pela \textbf{Entropia de Shannon}. Considerando o ficheiro como uma fonte de memória nula $X$ que gera símbolos $x \in \{0, \dots, 255\}$, a entropia de ordem-0 é definida por:

\begin{equation}
    H(X) = - \sum_{i=0}^{255} P(x_i) \log_2 P(x_i) \quad \text{[bits/símbolo]}
    \label{eq:shannon}
\end{equation}

onde $P(x_i)$ representa a probabilidade de ocorrência do símbolo $x_i$.

\subsubsection{Análise Global do Payload}

Aplicando a Equação~\ref{eq:shannon} à totalidade do \textit{payload} binário (excluindo o cabeçalho), obteve-se:

\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=Entropia de Ordem-0 Global]
\centering
$H(X) \approx 6.22$ bits/byte
\end{tcolorbox}

Este valor indica que, ignorando qualquer dependência entre bytes, a compressão máxima teórica seria de apenas $\sim 22\%$ (redução de 8 para 6.22 bits por byte). Trata-se de um resultado modesto que motivou a investigação de dependências inter-simbólicas.

\subsubsection{Análise de Correlação Sequencial}

Para investigar a existência de dependências sequenciais, calculou-se a \textbf{Entropia Condicional} de primeira ordem, que mede a incerteza de um símbolo $X_n$ dado o conhecimento do símbolo anterior $X_{n-1}$:

\begin{equation}
    H(X_n|X_{n-1}) = - \sum_{y \in \mathcal{X}} P(y) \sum_{x \in \mathcal{X}} P(x|y) \log_2 P(x|y)
    \label{eq:conditional}
\end{equation}

O resultado experimental obtido foi:

\begin{tcolorbox}[colback=green!5!white, colframe=green!75!black, title=Entropia Condicional de Primeira Ordem]
\centering
$H(X_n | X_{n-1}) \approx 5.36$ bits/byte
\end{tcolorbox}

O facto de $H(X|Y) < H(X)$ confirma a existência de correlação inter-simbólica (pelo teorema do condicionamento, que afirma que condicionar nunca aumenta a entropia). Contudo, o valor de 5.36 bits/byte permanece relativamente elevado, sugerindo que a correlação sequencial simples não é suficiente para explicar toda a redundância presente nos dados.

A nossa hipótese explicativa é que a natureza intercalada dos dados BF16 (MSB estruturado alternando com LSB ruidoso) "mascara"\ a verdadeira correlação entre os pesos adjacentes da rede neuronal.

\subsection{Análise Estrutural Diferenciada: \textit{Byte-Splitting}}

Para validar a hipótese de que a entropia está distribuída de forma desigual entre os componentes do formato BF16, procedeu-se à separação do fluxo de dados em dois canais distintos:

\begin{itemize}
    \item \textbf{Canal MSB}: Bytes nas posições ímpares (1, 3, 5, ...), contendo predominantemente o expoente e o bit de sinal;
    \item \textbf{Canal LSB}: Bytes nas posições pares (0, 2, 4, ...), contendo predominantemente a mantissa.
\end{itemize}

As entropias de ordem-0 foram recalculadas individualmente para cada canal, revelando uma disparidade notável:

\begin{table}[H]
    \centering
    \caption{Comparação de Entropia por Canal após \textit{Byte-Splitting}}
    \label{tab:split_results}
    \begin{tabular}{lccc}
        \toprule
        \textbf{Canal} & \textbf{Conteúdo Semântico} & \textbf{Entropia ($H$)} & \textbf{Característica} \\
        \midrule
        \textbf{MSB} & Expoente + Sinal & \textbf{2.71 bits/byte} & Altamente Estruturado \\
        \textbf{LSB} & Mantissa & \textbf{7.96 bits/byte} & Ruído Quase Uniforme \\
        \bottomrule
    \end{tabular}
\end{table}

Este resultado é particularmente significativo. O canal MSB apresenta uma entropia de apenas 2.71 bits/byte, representando um potencial de compressão de $66\%$ (de 8 para 2.71 bits). Em contraste, o canal LSB, com entropia de 7.96 bits/byte, aproxima-se do máximo teórico de 8 bits, indicando que os dados da mantissa se comportam essencialmente como ruído aleatório.

\subsubsection{Validação Visual: Histogramas de Frequência}

Os histogramas de frequência apresentados nas Figuras~\ref{fig:hist_msb} e~\ref{fig:hist_lsb} corroboram visualmente os valores numéricos obtidos.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{imagens/histogram_msb.png}
    \caption{Histograma do Byte Mais Significativo (MSB). Observa-se uma distribuição fortemente concentrada em torno de valores específicos, típica de pesos de redes neuronais normalizados. Esta concentração justifica o valor baixo de $H \approx 2.71$ bits/byte.}
    \label{fig:hist_msb}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{imagens/histogram_lsb.png}
    \caption{Histograma do Byte Menos Significativo (LSB). A distribuição aproxima-se da uniforme (plana), característica de dados com elevada aleatoriedade. Este comportamento explica a entropia de $H \approx 7.96$ bits/byte, muito próxima do máximo teórico.}
    \label{fig:hist_lsb}
\end{figure}

\subsubsection{Interpretação Física dos Resultados}

A diferença drástica entre as entropias dos dois canais tem uma explicação física fundamentada na natureza dos pesos de redes neuronais:

\begin{itemize}
    \item \textbf{Canal MSB (Expoente)}: Os pesos de LLMs são tipicamente valores pequenos, centrados em torno de zero, resultantes de técnicas de normalização (\textit{layer normalization}, \textit{weight decay}). Consequentemente, os expoentes concentram-se num intervalo reduzido de valores, gerando uma distribuição altamente previsível.
    
    \item \textbf{Canal LSB (Mantissa)}: A mantissa representa a precisão fracionária do peso. Para valores pequenos e normalizados, estes bits comportam-se como "ruído de quantização", com distribuição aproximadamente uniforme.
\end{itemize}

\subsection{Síntese e Estratégia de Compressão}

A análise realizada permite formular a seguinte observação crucial: a média das entropias separadas é $(2.71 + 7.96)/2 \approx 5.34$ bits/byte, um valor virtualmente idêntico à Entropia Condicional global (5.36 bits/byte). Isto sugere que a "memória"\ da fonte detetada na análise global resulta, na realidade, da estrutura interna do formato BF16 e não de correlação sequencial entre pesos adjacentes.

Com base nestes fundamentos teóricos e experimentais, definiu-se a seguinte estratégia de compressão em três etapas:

\begin{enumerate}
    \item \textbf{Pré-processamento (\textit{Split})}: Separar o fluxo de entrada em dois canais independentes (MSB e LSB), isolando a estrutura do ruído;
    
    \item \textbf{Canal MSB}: Aplicar codificação entrópica agressiva (Huffman ou Aritmética), explorando a baixa entropia ($H \approx 2.71$) para atingir rácios de compressão próximos de 3:1 neste canal;
    
    \item \textbf{Canal LSB}: Dado que $H \approx 8$ bits/byte, qualquer tentativa de compressão entrópica resultaria em expansão. Aplicar apenas compressão oportunística (RLE para sequências de zeros) ou armazenamento direto.
\end{enumerate}

% ----------------------------------------------------------------------------------
% SECÇÃO 3: BENCHMARKING DE COMPRESSORES EXISTENTES
% ----------------------------------------------------------------------------------
\section{Benchmarking de Compressores Existentes}
\label{sec:benchmark}

Antes de desenvolver uma solução especializada, é fundamental estabelecer um \textit{baseline} de referência através da avaliação de compressores de uso geral. Esta secção documenta os testes realizados com cinco compressores amplamente utilizados, medindo três métricas fundamentais: taxa de compressão, tempo de processamento e consumo de memória.

\subsection{Metodologia de Teste}

Os testes foram executados numa máquina com as seguintes características:
\begin{itemize}
    \item \textbf{CPU}: Intel® Core™ i7-1065G7 Processor (4 núcleos, 8 threads, até 3.9~GHz)
    \item \textbf{RAM}: 16~GB DDR4-2667 (SODIMM)
    \item \textbf{Armazenamento}: SSD NVMe (Samsung, 512~GB)
    \item \textbf{Sistema Operativo}: Ubuntu 24.04.3 LTS (64-bit), kernel Linux 6.8.0
\end{itemize}

Para cada compressor, foram medidos:
\begin{itemize}
    \item \textbf{Tamanho final}: Dimensão do ficheiro comprimido em MB;
    \item \textbf{Tempo de compressão}: Tempo real (\textit{wall-clock time}) em segundos;
    \item \textbf{Tempo de descompressão}: Tempo real em segundos;
    \item \textbf{Pico de RAM}: Consumo máximo de memória durante a operação.
\end{itemize}

O pico de RAM foi medido utilizando a ferramenta \texttt{/usr/bin/time -v}, que reporta o \textit{Maximum resident set size}.


\subsection{Resultados do Benchmarking}

A Tabela~\ref{tab:benchmark} apresenta os resultados consolidados para todos os compressores testados.

\begin{table}[H]
\centering
\caption{Desempenho de Compressores de Uso Geral no Ficheiro \texttt{model.safetensors}}
\label{tab:benchmark}
\begin{tabular}{lrrrrr}
\toprule
\textbf{Compressor} & \textbf{Tamanho} & \textbf{Rácio} & \textbf{T. Comp.} & \textbf{T. Decomp.} & \textbf{RAM (Comp.)} \\
\midrule
Original & 942 MB & 1.00:1 & --- & --- & --- \\
GZIP -1 & 754 MB & 1.25:1 & 40 s & 11 s & 1.9 MB \\
GZIP -9 & 746 MB & 1.26:1 & 104 s & 9 s & 1.9 MB \\
BZIP2 & 654 MB & \textbf{1.44:1} & 94 s & 49 s & 7.8 MB \\
XZ -9 & 660 MB & 1.43:1 & 933 s & 35 s & 675 MB \\
ZSTD -1 & 733 MB & 1.29:1 & \textbf{3 s} & \textbf{1 s} & 15 MB \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Análise dos Resultados}

\subsubsection{Taxa de Compressão}
O \textbf{BZIP2} obteve a melhor taxa de compressão (1.44:1), seguido de perto pelo \textbf{XZ} (1.43:1). Ambos utilizam algoritmos baseados em transformadas (\textit{Burrows-Wheeler} e \textit{LZMA}, respetivamente) que conseguem explorar padrões de longo alcance nos dados.

Os compressores baseados em LZ77/LZ78 (\textbf{GZIP} e \textbf{ZSTD}) apresentaram rácios inferiores (1.25--1.29:1), sugerindo que os padrões de repetição literal são menos prevalentes neste tipo de dados.

\subsubsection{Tempo de Processamento}
O \textbf{ZSTD} destacou-se claramente em velocidade, com apenas 3 segundos para compressão e 1 segundo para descompressão. Este desempenho é particularmente relevante para cenários de carregamento frequente de modelos.

O \textbf{XZ}, apesar do bom rácio de compressão, apresentou um tempo de compressão proibitivo de mais de 15 minutos, tornando-o impraticável para uso regular.

\subsubsection{Consumo de Memória}
O consumo de memória variou significativamente entre os compressores:
\begin{itemize}
    \item \textbf{GZIP}: Muito eficiente (1.9 MB), adequado para sistemas com recursos limitados;
    \item \textbf{BZIP2} e \textbf{ZSTD}: Consumo moderado (7.8--15 MB);
    \item \textbf{XZ}: Consumo elevado (675 MB), que pode ser problemático em sistemas com pouca RAM.
\end{itemize}

\subsection{Conclusões do Benchmarking}

Os resultados permitem identificar dois pontos de operação de referência:

\begin{enumerate}
    \item \textbf{Máxima compressão}: BZIP2 com rácio 1.44:1, tempo aceitável (94 s) e consumo moderado de RAM;
    \item \textbf{Máxima velocidade}: ZSTD com rácio 1.29:1, tempo excelente (3 s) e consumo baixo de RAM.
\end{enumerate}

O objetivo do codec a desenvolver será \textbf{superar o BZIP2 em taxa de compressão} mantendo tempos competitivos com o ZSTD.

% ----------------------------------------------------------------------------------
% SECÇÃO 4: IMPLEMENTAÇÃO DO CODEC
% ----------------------------------------------------------------------------------
\section{Implementação do Codec}
\label{sec:implementacao}

Esta secção descreve em detalhe a arquitetura e implementação do codec desenvolvido, desde as decisões de engenharia de alto nível até aos algoritmos específicos utilizados em cada módulo.

\subsection{Arquitetura Geral}

O codec implementa uma arquitetura \textit{split-stream} composta por três módulos principais:

\begin{enumerate}
    \item \textbf{Módulo de Pré-processamento (\textit{Splitter})}: Separa o fluxo de entrada em dois canais independentes;
    \item \textbf{Módulo de Compressão MSB}: Aplica codificação entrópica ao canal de expoentes;
    \item \textbf{Módulo de Compressão LSB}: Aplica compressão oportunística ao canal de mantissas.
\end{enumerate}

O processamento é realizado em blocos de 1 MB para controlar o consumo de memória e permitir a paralelização futura.

\subsection{Formato do Ficheiro Comprimido}

O ficheiro de saída (\texttt{.sc} -- \textit{SafeTensors Compressed}) possui a seguinte estrutura:

\begin{verbatim}
[Header Size: 8 bytes]
[Header JSON: variável]
[Mode Flag: 1 byte (0=FAST, 1=BEST)]
[Bloco 1]
[Bloco 2]
...
[Bloco N]
\end{verbatim}

Cada bloco possui a seguinte estrutura interna:

\begin{verbatim}
[sz_m: 4 bytes (tamanho do pacote MSB)]
[sz_l: 4 bytes (tamanho do pacote LSB)]
[Pacote MSB: sz_m bytes]
[Pacote LSB: sz_l bytes]
\end{verbatim}

O cabeçalho JSON original é preservado integralmente para garantir a compatibilidade com ferramentas existentes. O \textit{mode flag} permite ao descodificador identificar automaticamente o algoritmo utilizado.

\subsection{Estratégia para o Canal MSB}

O canal MSB, contendo os expoentes e bits de sinal, foi identificado como a principal oportunidade de compressão. Esta subsecção descreve o processo iterativo de otimização.

\subsubsection{Avaliação de Técnicas de Predição}

A literatura de compressão de dados sugere frequentemente o uso de codificação preditiva para reduzir a variância dos resíduos. Foram testadas duas abordagens:

\paragraph{Preditor Linear (Delta)}
A primeira abordagem utilizou um preditor de primeira ordem clássico:
\begin{equation}
    r_n = (x_n - x_{n-1}) \mod 256
\end{equation}

\textbf{Resultado}: A entropia \textbf{aumentou} de 2.70 para 3.28 bits/byte (ganho negativo de -0.58 bits).

\textbf{Análise}: Este comportamento paradoxal deve-se ao bit de sinal do BF16. Quando os pesos oscilam entre valores positivos e negativos pequenos (comum em LLMs normalizados), o bit de sinal alterna frequentemente. A subtração aritmética interpreta esta alternância como "saltos"\ de grande magnitude, dispersando o histograma dos resíduos.

\paragraph{Preditor XOR}
Para mitigar o problema do bit de sinal, testou-se um preditor baseado em XOR:
\begin{equation}
    r_n = x_n \oplus x_{n-1}
\end{equation}

\textbf{Resultado}: Entropia de 3.11 bits/byte, ainda superior à original.

\subsubsection{Decisão de Engenharia}

Concluiu-se que a baixa entropia do canal MSB não advém de correlação sequencial ($x_n \approx x_{n-1}$), mas sim da \textbf{distribuição global estática} dos expoentes (concentração em valores específicos). Qualquer transformação preditiva simples tende a destruir esta estrutura estatística favorável.

Consequentemente, optou-se por \textbf{codificar diretamente os valores brutos} do canal MSB, sem transformação prévia.

\subsubsection{Codificação de Entropia}

Foram implementados e comparados dois algoritmos de codificação entrópica:

\paragraph{Codificação de Huffman (Estática)}
Implementação clássica com tabela de códigos pré-calculada (\textit{Look-Up Table}) para codificação eficiente. Cada bloco inclui a tabela de frequências (256 × 4 bytes = 1 KB), permitindo a reconstrução da árvore de Huffman e descodificação independente de blocos.

A codificação utiliza um \textit{buffer} de bits acumulado em variável de 64 bits, emitindo bytes completos para o fluxo de saída à medida que ficam disponíveis. Esta técnica elimina operações bit-a-bit individuais, maximizando o desempenho.

\paragraph{Codificação Aritmética}
Implementação de codificação aritmética, uma técnica de codificação entrópica que oferece compressão próxima da entropia teórica. As principais características da implementação são:

\begin{itemize}
    \item \textbf{Precisão}: Utiliza aritmética de 32 bits com intervalos $[low, high]$ normalizados;
    \item \textbf{Buffer de 64 bits}: Acumulação eficiente de bits com \textit{flush} em lotes para minimizar operações de I/O;
    \item \textbf{Pending bits}: Gestão de bits pendentes para tratar situações de \textit{underflow} durante a renormalização.
\end{itemize}

A Tabela~\ref{tab:entropy_bench} compara o desempenho dos dois algoritmos implementados:

\begin{table}[H]
    \centering
    \caption{Comparação de Algoritmos de Codificação Entrópica (Canal MSB)}
    \label{tab:entropy_bench}
    \begin{tabular}{lccc}
        \toprule
        \textbf{Algoritmo} & \textbf{Tamanho Final} & \textbf{Tempo Cod.} & \textbf{Tempo Dec.} \\
        \midrule
        Huffman (LUT) & x MB & \textbf{x s} & \textbf{x s} \\
        Aritmética & \textbf{x MB} & x s & x s \\
        \bottomrule
    \end{tabular}
\end{table}

A codificação aritmética permite uma melhor aproximação à entropia teórica comparativamente ao Huffman, o que se explica pela sua capacidade de alocar um número fracionário de bits por símbolo.

\subsection{Estratégia para o Canal LSB}

O canal LSB, com entropia de 7.96 bits/byte, apresenta características de ruído quase uniforme. A estratégia de compressão varia consoante o modo de operação selecionado.

\subsubsection{Modo FAST: Armazenamento Direto (Raw)}

No modo FAST, optou-se por uma estratégia de armazenamento direto para o canal LSB:

\begin{itemize}
    \item A entropia do canal LSB ($\approx 7.96$ bits/byte) está muito próxima do máximo teórico de 8 bits/byte;
    \item Qualquer algoritmo de compressão entrópica introduziria \textit{overhead} de metadados (tabelas de frequência, marcadores) que anularia eventuais ganhos;
    \item A cópia direta (\textit{Raw}) é computacionalmente eficiente e não introduz latência adicional.
\end{itemize}

\textbf{Decisão de Engenharia}: No modo FAST, os dados do canal LSB são copiados diretamente para o ficheiro comprimido sem qualquer transformação. Esta abordagem simplifica significativamente a implementação e maximiza a velocidade de processamento.

\subsubsection{Modo BEST: Codificação Aritmética}

No modo BEST, o canal LSB é também comprimido utilizando codificação aritmética, apesar da sua entropia elevada. Esta decisão justifica-se por:

\begin{itemize}
    \item Embora os ganhos sejam marginais, a codificação aritmética não expande os dados significativamente mesmo para fontes de alta entropia;
    \item O processamento paralelo (MSB e LSB em \textit{threads} separadas) mitiga o custo computacional adicional;
    \item Permite obter a taxa de compressão máxima possível para utilizadores que privilegiam o tamanho sobre a velocidade.
\end{itemize}

\subsection{Modos de Operação}

Para satisfazer o requisito de múltiplos pontos de operação, o codec oferece dois modos:

\begin{tcolorbox}[colback=yellow!10!white, colframe=yellow!75!black, title=Modos de Operação]
\begin{itemize}
    \item \textbf{Modo FAST}: Huffman com LUT (MSB) + Raw (LSB) \\
    \textit{Otimizado para velocidade de codificação e descodificação}
    
    \item \textbf{Modo BEST}: Aritmética (MSB) + Aritmética (LSB) em paralelo \\
    \textit{Maximiza a taxa de compressão utilizando processamento paralelo com \texttt{std::async}}
\end{itemize}
\end{tcolorbox}

No modo BEST, a compressão dos canais MSB e LSB é executada em paralelo utilizando \texttt{std::async}, permitindo explorar múltiplos núcleos do processador e mitigando o custo computacional adicional da codificação aritmética.

\subsection{Gestão de Memória}

O codec foi desenhado para operar com consumo de memória controlado:

\begin{itemize}
    \item \textbf{Processamento por blocos}: Cada bloco de 1 MB é processado independentemente, evitando a necessidade de carregar o ficheiro completo em memória;
    \item \textbf{Buffers reutilizáveis}: Os \textit{buffers} de entrada/saída são alocados uma única vez e reutilizados para todos os blocos;
    \item \textbf{Pico de RAM estimado}: Aproximadamente 10 MB para o codec, independentemente do tamanho do ficheiro de entrada.
\end{itemize}

% ----------------------------------------------------------------------------------
% SECÇÃO 5: RESULTADOS
% ----------------------------------------------------------------------------------
\section{Resultados Experimentais}
\label{sec:resultados}

Para avaliar o desempenho do codec desenvolvido, foram realizados testes exaustivos em cinco ficheiros \texttt{.safetensors} de diferentes dimensões, representando modelos de linguagem variados. Os testes incluíram ambos os modos de operação e verificaram a integridade dos dados através de \textit{hashes} MD5.

\subsection{Conjunto de Dados de Teste}

A Tabela~\ref{tab:test_files} descreve os ficheiros utilizados nos testes:

\begin{table}[H]
    \centering
    \caption{Ficheiros de Teste Utilizados}
    \label{tab:test_files}
    \begin{tabular}{lrr}
        \toprule
        \textbf{Ficheiro} & \textbf{Tamanho} & \textbf{Descrição} \\
        \midrule
        \texttt{model.safetensors} & 943 MB & Qwen2-0.5B (modelo principal) \\
        \texttt{model\_1.safetensors} & 453 MB & Modelo auxiliar 1 \\
        \texttt{model\_2.safetensors} & 583 MB & Modelo auxiliar 2 \\
        \texttt{model\_3.safetensors} & 2.1 GB & Modelo auxiliar 3 \\
        \texttt{model\_4.safetensors} & 3.8 GB & Modelo de grande escala \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Resultados Consolidados}

A Tabela~\ref{tab:testes_resultados} apresenta os resultados completos para todos os modelos e modos de operação:

\begin{table}[H]
    \centering
    \caption{Resultados dos Testes de Compressão para Todos os Modelos}
    \label{tab:testes_resultados}
    \begin{tabular}{lcccccc}
        \toprule
        \textbf{Modelo} & \textbf{Modo} & \textbf{Original} & \textbf{Comprimido} & \textbf{Rácio} & \textbf{T. Cod.} & \textbf{T. Dec.} \\
        \midrule
        \multirow{2}{*}{\texttt{model.safetensors}} 
        & Fast & x MB & x MB & x:1 & x s & x s \\
        & Best & x MB & x MB & x:1 & x s & x s \\
        \midrule
        \multirow{2}{*}{\texttt{model\_1.safetensors}} 
        & Fast & x MB & x MB & x:1 & x s & x s \\
        & Best & x MB & x MB & x:1 & x s & x s \\
        \midrule
        \multirow{2}{*}{\texttt{model\_2.safetensors}} 
        & Fast & x MB & x MB & x:1 & x s & x s \\
        & Best & x MB & x MB & x:1 & x s & x s \\
        \midrule
        \multirow{2}{*}{\texttt{model\_3.safetensors}} 
        & Fast & x GB & x GB & x:1 & x s & x s \\
        & Best & x GB & x GB & x:1 & x s & x s \\
        \midrule
        \multirow{2}{*}{\texttt{model\_4.safetensors}} 
        & Fast & x GB & x GB & x:1 & x s & x s \\
        & Best & x GB & x GB & x:1 & x s & x s \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Verificação de Integridade}

Em todos os testes, a integridade dos dados foi verificada através de comparação de \textit{hashes} MD5:

\begin{verbatim}
$ md5sum model.safetensors
d7baf050ec13cb76a756d0d344f28447  model.safetensors

$ ./decoder model.sc model_restored.safetensors
$ md5sum model_restored.safetensors
d7baf050ec13cb76a756d0d344f28447  model_restored.safetensors  [MATCH]
\end{verbatim}

\textbf{Resultado}: 100\% dos testes confirmaram compressão sem perdas.


\subsection{Comparação com Benchmarks}

A Tabela~\ref{tab:comparison} compara o codec desenvolvido com os compressores de referência para o ficheiro principal:

\begin{table}[H]
    \centering
    \caption{Comparação do Codec Desenvolvido com Compressores de Uso Geral}
    \label{tab:comparison}
    \begin{tabular}{lccc}
        \toprule
        \textbf{Compressor} & \textbf{Rácio} & \textbf{Tempo Comp.} & \textbf{Tempo Decomp.} \\
        \midrule
        BZIP2 & 1.44:1 & 94 s & 49 s \\
        XZ -9 & 1.43:1 & 933 s & 35 s \\
        ZSTD -1 & 1.29:1 & 3 s & 1 s \\
        \midrule
        \textbf{Codec (Fast)} & x:1 & x s & x s \\
        \textbf{Codec (Best)} & x:1 & x s & x s \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Análise e Discussão}

\subsubsection{Taxa de Compressão}

O codec desenvolvido \textbf{superou todos os compressores de uso geral testados}, atingindo um rácio de x:1 no modelo principal, comparado com 1.44:1 do melhor compressor genérico (BZIP2). Esta melhoria representa uma poupança adicional significativa no ficheiro original.

A superioridade do codec especializado deve-se à exploração direta da estrutura do formato BF16, que os compressores genéricos tratam como dados opacos.

\subsubsection{Variabilidade entre Modelos}

Observou-se uma variação significativa nos rácios de compressão entre diferentes modelos:
\begin{itemize}
    \item \textbf{\texttt{model.safetensors}}: Rácio elevado ($\approx$ x:1);
    \item \textbf{\texttt{model\_1/2/3.safetensors}}: Rácio moderado ($\approx$ x:1);
    \item \textbf{\texttt{model\_4.safetensors}}: Rácio reduzido ($\approx$ x:1).
\end{itemize}


Esta variação sugere que diferentes arquiteturas de rede neuronal apresentam distribuições de pesos distintas. Modelos com pesos mais concentrados (menor variância de expoentes) beneficiam mais da estratégia de \textit{split-stream}.

\subsubsection{Desempenho Temporal}

Ambos os modos oferecem desempenho consistente após a otimização do decoder Huffman, com LUT:
\begin{itemize}
    \item \textbf{Modo Fast}: Para o modelo principal (\texttt{model.safetensors}), a codificação foi concluída em x~s e a descodificação em x~s;
    \item \textbf{Modo Best}: A codificação aumentou para x~s e a descodificação para x~s, proporcionando uma melhoria marginal na taxa de compressão.
\end{itemize}

Comparativamente aos compressores de uso geral, o codec desenvolvido apresenta:
\begin{itemize}
    \item \textbf{Desempenho temporal superior ao BZIP2 e XZ} em todos os cenários testados, tanto na codificação como na descodificação;
    \item \textbf{Taxa de compressão superior ao ZSTD}, atingindo até 1.49:1 no modelo principal, à custa de um maior tempo de processamento.
\end{itemize}

\subsubsection{Comparação do Pico de RAM}

A Tabela~\ref{tab:ram_peak} mostra o uso máximo de memória residente (RAM) durante a codificação e decodificação dos modelos usando os modos \textbf{BEST} e \textbf{FAST}.

\begin{table}[h!]
\centering
\begin{tabular}{lcccc}
\hline
\textbf{Modelo} & \textbf{BEST Cod. (kB)} & \textbf{BEST Dec. (kB)} & \textbf{FAST Cod. (kB)} & \textbf{FAST Dec. (kB)} \\
\hline
model\_1 & x & x & x & x \\
model\_2 & x & x & x & x \\
model\_3 & x & x & x & x \\
model & x & x & x & x \\
model\_4 & x & x & x & x \\
\hline
\end{tabular}
\caption{Pico de RAM (em kB) durante codificação e decodificação para cada modelo.}
\label{tab:ram_peak}
\end{table}

Observa-se que o pico de RAM é similar entre os modos, com variações pequenas dependendo do tamanho do modelo e do método de compressão. Em geral, o modo \textbf{BEST} tende a consumir ligeiramente mais memória durante a decodificação.

\subsubsection{Escalabilidade}

Os tempos de processamento escalam linearmente com o tamanho do ficheiro:
\begin{itemize}
    \item \texttt{model.safetensors} (x~MB): x~s de codificação e x~s de descodificação (Fast);
    \item \texttt{model\_4.safetensors} (x~GB): x~s de codificação e x~s de descodificação (Fast).
\end{itemize}

Apesar do aumento absoluto dos tempos, o \textit{throughput} mantém-se da mesma ordem de grandeza, variando aproximadamente entre x e x~MB/s, o que demonstra boa escalabilidade do codec para modelos de maior dimensão.



% ----------------------------------------------------------------------------------
% SECÇÃO 6: CONCLUSÕES
% ----------------------------------------------------------------------------------
\section{Conclusões}
\label{sec:conclusoes}

\subsection{Síntese do Trabalho Realizado}

Este trabalho desenvolveu um codec especializado para a compressão sem perdas de pesos de modelos de linguagem armazenados no formato SafeTensors/BF16. A abordagem metodológica baseou-se numa análise profunda da estrutura estatística dos dados, culminando numa arquitetura \textit{split-stream} que processa separadamente os bytes de expoente/sinal (MSB) e mantissa (LSB).

\subsection{Principais Contribuições}

\begin{enumerate}
    \item \textbf{Caracterização da Fonte}: Demonstrou-se que a entropia do formato BF16 está distribuída de forma altamente desigual entre os seus componentes (2.71 vs. 7.96 bits/byte), fundamentando a estratégia de separação de canais.
    
    \item \textbf{Superação dos Benchmarks}: O codec desenvolvido atingiu um rácio de compressão de x:1, superando todos os compressores de uso geral testados, incluindo o BZIP2 (1.44:1).
    
    \item \textbf{Eficiência Computacional}: O decoder Huffman otimizado com LUT de 12 bits atinge \textit{throughput} elevado, permitindo ciclos completos de codificação + descodificação em tempos competitivos para ficheiros de grande dimensão.
    
    \item \textbf{Múltiplos Pontos de Operação}: A disponibilização de dois modos (Fast e Best) permite ao utilizador escolher o compromisso ideal para o seu caso de uso.
\end{enumerate}

\subsection{Limitações e Trabalho Futuro}

Identificaram-se as seguintes limitações e oportunidades de melhoria:

\begin{itemize}
    \item \textbf{Variabilidade entre modelos}: Os rácios de compressão variam significativamente entre diferentes arquiteturas de rede neuronal. Uma extensão interessante seria a implementação de um mecanismo adaptativo que selecione a estratégia ótima com base em estatísticas do bloco.
    
    \item \textbf{Canal LSB}: O armazenamento direto (Raw) é a única estratégia viável para dados com entropia próxima de 8 bits/byte. Técnicas mais sofisticadas, como a exploração de correlações intra-tensor ou transformadas de domínio, poderiam eventualmente reduzir a entropia efetiva deste canal.
    
    \item \textbf{Paralelização}: A arquitetura por blocos é intrinsecamente paralelizável. Uma implementação \textit{multi-threaded} poderia reduzir significativamente os tempos de processamento em sistemas \textit{multi-core}.
    
    \item \textbf{Compressores especializados}: A comparação com ferramentas específicas para \textit{floating-point} (e.g., \texttt{fpzip}, \texttt{zfp}) forneceria uma perspetiva adicional sobre o estado da arte.
\end{itemize}

\subsection{Considerações Finais}

O trabalho demonstrou que a compressão eficiente de dados estruturados beneficia significativamente de uma análise prévia das suas propriedades estatísticas. A estratégia de \textit{byte-splitting}, embora conceptualmente simples, permitiu explorar a estrutura inerente ao formato BF16 de forma que os compressores genéricos não conseguem.

Os resultados obtidos validam a abordagem \textit{domain-specific} para a compressão de pesos de LLMs, com implicações práticas relevantes para o armazenamento e distribuição de modelos de grande escala.

\end{document}